---
title: "Methodology"
author: "Stephen Pelkofer"
date: "6/13/2020"
output:
  html_document:
    theme: lumen
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Beer Me Methodology

In the "Beer Me" analysis, several steps were performed before reaching the end outputs on the "Beer Comparisons" tab. At a high level
here was the process:

1. Scrape the data from MolsonCoors, which can be found here: [molsoncoors.com](https://www.molsoncoors.com/sites/molsonco/files/Combined%20Website%20Update%2006112020_0.pdf)
  - This data was *messy* - as it was free text in a PDF
  - The scraping process was not as automated as I had hoped for, but it was certainly faster than manual copying and pasting
2. Cleaning the data
  - Some basic EDA and feature selection
  - Removed variables with zero or low variance
  - Combined ingredients that were similar
  - Created dummy-variables from the ingredients
  - The end result was a 222x186 dataset
3. Partioning Around Medoids (PAM) cluster algorithm
  - Unsupervised algorithm similar to k-means
  - Added cluster labels to the dataset
4. Uniform Manifold Approximation and Projection (UMAP)
  - Dimensionality reduction technique
  - Very good at maintainig local and global structure of the data
5. Nearest Neighbors
  - Extract nearest neighbors for each beer
  - These neighbors are the recommendations
  
As parts 1 and 2 are boring, I will focus mainly on the methodology pieces in parts 3,4, and 5.

# Libraries & Data

The following packages are needed.

```{r message = FALSE}
library(tidyverse)
library(caret)
library(ggtext)
library(tidyr)
library(plotly)
library(StatMatch)
library(cluster)
library(uwot)
```

```{r  include = FALSE, message = FALSE}
beer_data <- read_csv("../data-raw/CleanedBeerData.csv")
beer_full <- beer_data %>%
  select(!c("Calories_from_fat","Cholesterol_mg","Fat_grams","Saturated_fat_grams","Trans_fat_grams","Fiber_grams"))
```

# Partitioning around Medoids

Partition around medics (PAM) is an algorithm that is intended to find objects (medoids) that are centrally located in clusters. The goal of the algorithm is to minimize the average dissimilarity of objects to their closest selected object. 

I used the PAM clustering algorithm on the full beer dataset. Before I ran the model, I had to create a distance matrix form the data. The distance matric has pairwise distances for each datapoint. I used the [gower distance]("https://medium.com/@rumman1988/clustering-categorical-and-numerical-datatype-using-gower-distance-ab89b3aa90d9#:~:text=Gower%20Distance%20is%20a%20distance,of%20categorical%20and%20numerical%20values."), because I had a mix of numerical and categorical variables.

```{r}
# Gower Dist
cluster_data_input <- beer_full %>% select(!c("Brand","Brand_Style","Ingredients")) # Don't include raw text columns
cluster_dist_input <- dist(cluster_data_input, method = "gower")
```

For the PAM cluster, I chose k = 10 (clusters) based on the [Silhouette method](https://en.wikipedia.org/wiki/Silhouette_(clustering)), which is a measure of how similar objects are to their cluster label. Values for k = 5:20 produced similar silhouette scores, but I chose 10 based on some domain knowledge of the beer data set.

```{r}
pam_cluster <- pam(x = cluster_dist_input, k = 10, diss = TRUE, cluster.only = TRUE)
beer_data_with_clusters <- beer_full %>%
  mutate(Ingr_Cluster = as.factor(pam_cluster))
```

There isn't a ton we can do with the cluster labels yet. Ideally, I want to be able to visualize the beers on a scatterplot, and be able to recommend 5 other beers, based on any selection. This is where UMAP comes in.


# Uniform Manifold Approximation and Projection (UMAP)

UMAP is a dimensionality reduction technique that uses neighbor graphs to project data down to a lower space. In simple terms, I want to go from a 222x177 (222 beers, 177 features) matrix to a 222x2 matrix. If I can project the dataset into 2 dimensions, I can visualize all the beers on a scatterplot.

One of the cool things about UMAP is that it tries to optimize for both the local and global structures; something t-SNE (another dimensionality reduction technique) does not do well. This is important because when we visualize the UMAP output, there might be two clusters of beers close to eachother. The two clusters might have some similarities (high ABV, high calories, fruity, etc.).

For more technical details on UMAP, check out this awesome talk from PyData [Modern Approaches to Dimension Reduction](https://www.youtube.com/watch?v=YPJQydzTLwQ).

I'm using the [uwot package](https://github.com/jlmelville/uwot) implementation of UMAP. This is one of my favorite implementations of UMAP becasue for a smaller dataset (n < 4096), the UMAP algorithm allows you to extract the exact nearest neighors. I only have 222 beers in the data, so the nearest neighbors will be the "Beer Me" recommendations.

The code below shows the UMAP implementations. I've included the cluster labels as a target variable, and gave that variable a 50% weight. This is not mandatory - I don't even have to use a target variable, but the cluster labels likley provide meaningful information in are otherwise sparse data set.

```{r}
## UMAP
umap_data_input <- beer_data_with_clusters %>% 
  select(!contains("Brand")) %>%
  select(!Ingredients)

umap_output <- uwot::umap(umap_data_input,
                          y = umap_data_input$Ingr_Cluster,
                          target_weight = .5, # 50% on the target var (cluster label)
                          metric = "euclidean",
                          n_neighbors = 6,
                          n_components = 2,
                          spread = .75,
                          min_dist = .01,
                          scale = TRUE,
                          ret_nn = TRUE) # retain the nearest neighbors
```

For more information of the hypyerparameters that you can tune in uwot::umap(), check out this link [UMAP hyperparameters](https://rdrr.io/cran/uwot/man/umap.html).

Plotting UMAP with the cluster labels...

```{r}
# Plotting UMAP
umap_plotting_data <- cbind(beer_data_with_clusters,umap_output$embedding)
umap_plotting_data <- umap_plotting_data %>% rename(UMAP_X = `1`,UMAP_Y = `2`)
gg_umap <- ggplot(data = umap_plotting_data,aes(x = UMAP_X,
                                                y = UMAP_Y,
                                                colour = Ingr_Cluster,
                                                text = paste0(Brand,"<br>",Brand_Style,"<br>","ABV: ",ABV))) +
  geom_point(colour = "#fe9929",
             alpha = .6) +
  labs(title = "UMAP 2-D Respresentation of Beer Data") +
  theme(plot.title = element_markdown(size = 14),
        plot.subtitle = element_markdown(size = 10),
        plot.caption = element_markdown(),
        legend.position = "top",
        panel.background = element_rect(fill = "white", colour = "white"),
        panel.grid.major = element_line(colour = "#f0f0f0",size = .1)) + 
  geom_jitter(width = 1.5, height = 1.5,alpha = .6)

ggplotly(gg_umap, tooltip = "text") %>%
            config(displayModeBar = F)
```

Becasue it's a smaller dataset, UMAP can extract the nearest neighbors...

```{r}
# Extract Nearest Neigbor Data
beer_nn_data <- cbind(beer_data %>% select("Brand","Brand_Style"),umap_output$nn)

beer_nn_data <- beer_nn_data %>%
  pivot_longer(starts_with("euclidean"),
               names_to = c(".value", "set"),
               names_pattern = "(euclidean\\.[a-z]*\\.)(.)")
  
names(beer_nn_data) <- c("Brand","Brand_Style","Neighbor_Rk","Neighbor_Idx","Neighbor_Dist")
```

Example... Miller lite
Same kable table from Beer Comparison's tab...

```{r}

```

